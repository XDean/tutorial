当我们学习一门编程语言时，第一个程序就是`Hello World`。

而在大数据的世界里，最流行的入门程序当属`Word Count`。

需求很简单，现在我们有一个文本文件，我们要统计文本中每个单词的出现次数。

你可以选择下载我准备的[`shakespeare.txt`文件](/spark/resources/shakespeare.txt)(这是一本莎翁全集，右键另存为)，或是选用任何文本文件。

打开`pyspark`交互式命令行，并运行以下代码。

**先别慌！我知道突如其来的代码多少显得有些仓促。但是在第一节课你不需要了解其中细节。**

```python
lines = sc.textFile("shakespeare.txt")
counts = lines.flatMap(lambda line: line.split()) \
              .map(lambda word: (word, 1)) \
              .reduceByKey(lambda a, b: a + b) \
              .sortBy(lambda e: e[1], ascending=False)
counts.take(5)
```

运行代码后你将会看到如下输出

```shell
[('the', 25815), ('I', 20402), ('and', 19249), ('to', 17222), ('of', 16526)]
```

可以看到，Spark已经正确输出了我们想要的结果。恭喜你运行了你的第一个Spark程序。

## DataFrame版本

然而Hello World还有另一个版本，代码如下

```python
import pyspark.sql.functions as F

lines = spark.read.text("shakespeare.txt")
words = lines \
    .select(F.explode(F.split(lines.value, r"\s")).name("word")) \
    .where(F.length("word") > 0)
counts = words \
    .groupBy("word") \
    .agg(F.count("*").name("count")) \
    .orderBy(F.col("count").desc())

counts.show(5)
```

运行代码后你将会看到如下输出

```text
+----+-----+
|word|count|
+----+-----+
| the|25815|
|   I|20402|
| and|19249|
|  to|17222|
|  of|16526|
+----+-----+
only showing top 5 rows
```

可以看到，Spark已经正确输出了我们想要的结果，并且`show`出来的结果更加可读。

这两个Hello World其实分别是Spark的两种主要API：RDD和DataFrame。

后文将介绍它们的关系和区别。在此之前，你可以先再回头看看两段代码，观察它们的代码风格，同时还可以感受一下它们的执行速度。